{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587884d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import cv2\n",
    "import imutils\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33625cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMerger:\n",
    "    \"\"\"\n",
    "    ImageMerger can merge images from a dataset including images collected by a drone with the purpose of detecting \n",
    "    moving people in the images.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, mask_file):\n",
    "        \"\"\"\n",
    "        :param data_dir: path to the directory where the data is located\n",
    "        :param mask_file: path of a mask which removes unneccesary text from the images\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get all directories inside the data_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.dirs = os.listdir(data_dir)\n",
    "        self.mask = np.array(cv2.imread(mask_file)) // 255\n",
    "        \n",
    "        # Store the merged images\n",
    "        self.merged_images = {}\n",
    "        \n",
    "    def load_images(self):\n",
    "        \"\"\"\n",
    "        Loads the images and homographies from one directory at a time and yields them\n",
    "        \"\"\"\n",
    "        \n",
    "        for a_dir in self.dirs:\n",
    "            if a_dir == 'labels.json':\n",
    "                continue\n",
    "            input_images = {}\n",
    "            \n",
    "            # Load all images in the train directory\n",
    "            image_dir_file_path = os.path.join(os.path.join(self.data_dir, a_dir), '*.png')\n",
    "            images = glob.glob(image_dir_file_path)\n",
    "            \n",
    "            # Copy all the raw images to the input images dict\n",
    "            for im in images:\n",
    "                file = os.path.basename(im)\n",
    "                img_name = os.path.splitext(file)[0]\n",
    "                src = cv2.cvtColor(cv2.imread(im), cv2.COLOR_BGR2RGB)\n",
    "                #src = cv2.imread(im, cv2.IMREAD_GRAYSCALE)\n",
    "                src = self.apply_mask(src)\n",
    "                input_images[img_name] = src\n",
    "            \n",
    "            # Load the homographies json file\n",
    "            homographies_file_path = os.path.join(os.path.join(self.data_dir, a_dir), 'homographies.json')\n",
    "            with open(homographies_file_path, 'rb') as f:\n",
    "                homographies = json.load(f)\n",
    "            \n",
    "            yield a_dir, input_images, homographies\n",
    "        \n",
    "    def apply_mask(self, image):\n",
    "        \"\"\"\n",
    "        Applys the provided mask on image\n",
    "        :param image: image file which will be treated as np.array\n",
    "        \"\"\"\n",
    "\n",
    "        return image * self.mask\n",
    "    \n",
    "    def merge_images(self, alpha=0.1, axis=0, method='cltr', debug=False):\n",
    "        \"\"\"\n",
    "        Merges all the images given the homographies which are retrieved from load_data. \n",
    "        :param alpha: Weighting for merging the images. The image to be merged on will be considered with a weight a alpha,\n",
    "        the second image with a weight of 1-alpha.\n",
    "        :param axis: defines the axis along which the images shall be merged. 0: camera axis, 1: time axis\n",
    "        :param method: defines in which order the images shall be merged:\n",
    "            - cltr: -B01 (along camera axis)/index 3 (along time axis) is the base image, the other images are merged on it\n",
    "                    from left to right along the camera axis or from the first in time (index 0) to the last in time (index\n",
    "                    6)\n",
    "            - crtl: -B01 (along camera axis)/index 3 (along time axis) is the base image, the other images are merged on it\n",
    "                    from right to left along the camera axis or from the last in time (index 6) to the first in time (index\n",
    "                    0)\n",
    "            - coutl: -B01 (along camera axis)/index 3 (along time axis) is the base image, the other images are merged on \n",
    "                    it from the center out switching between the left and right side of the center (along camera axis)/\n",
    "                    previous and next in time (along time axis) starting with the left side/previous in time. \n",
    "            - coutr: -B01 (along camera axis)/index 3 (along time axis) is the base image, the other images are merged on \n",
    "                    it from the center out switching between the left and right side of the center (along camera axis)/\n",
    "                    previous and next in time (along time axis) starting with the right side/next in time.\n",
    "            - cinl: -B01 (along camera axis)/index 3 (along time axis) is the base image, the other images are merged on \n",
    "                    it from the outside inward switching between the left and right side of the center (along camera axis)/\n",
    "                    previous and next in time (along time axis) starting with the left side/previous in time. \n",
    "            - cinr: -B01 (along camera axis)/index 3 (along time axis) is the base image, the other images are merged on \n",
    "                    it from the outside inward switching between the left and right side of the center (along camera axis)/\n",
    "                    previous and next in time (along time axis) starting with the right side/next in time.\n",
    "        :param debug: if True, results are printed to be able to debug \n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the keys and indices depending on axis and method\n",
    "        keys = []\n",
    "        indices = []\n",
    "        base = None\n",
    "        if axis == 0:\n",
    "            indices = range(0, 7)\n",
    "            if method == 'cltr':\n",
    "                base = '-B01'\n",
    "                keys = ['-B05', '-B04', '-B03', '-B02', '-G01', '-G02', '-G03', '-G04', '-G05']\n",
    "            elif method == 'crtl': \n",
    "                base = '-B01'\n",
    "                keys = ['-G05', '-G04', '-G03', '-G02', '-G01', '-B02', '-B03', '-B04', '-B05']\n",
    "            elif method == 'coutl':\n",
    "                base = '-B01'\n",
    "                keys = ['-B02', '-G01', '-B03', '-G02', '-B04', '-G03', '-B05', '-G04', '-G05']\n",
    "            elif method == 'coutr':\n",
    "                base = '-B01'\n",
    "                keys = ['-G01', '-B02', '-G02', '-B03', '-G03', '-B04', '-G04', '-B05', '-G05']\n",
    "            elif method == 'cinl':\n",
    "                base = '-B01'\n",
    "                keys = ['-B05', '-G05', '-B04', '-G04', '-B03', '-G03', '-B02', '-G02', '-G01']\n",
    "            elif method == 'cinr':\n",
    "                base = '-B01'\n",
    "                keys = ['-G05', '-B05', '-G04', '-B04', '-G03', '-B03', '-G02', '-B02', '-G01']\n",
    "        elif axis == 1:\n",
    "            indices = ['-B05', '-B04', '-B03', '-B02', '-B01', '-G01', '-G02', '-G03', '-G04', '-G05']\n",
    "            if method == 'cltr':\n",
    "                base = 3\n",
    "                keys = [0, 1, 2, 4, 5, 6]\n",
    "            elif method == 'crtl': \n",
    "                base = 3\n",
    "                keys = [6, 5, 4, 2, 1, 0]\n",
    "            elif method == 'coutl':\n",
    "                base = 3\n",
    "                keys = [2, 4, 1, 5, 0, 6]\n",
    "            elif method == 'coutr':\n",
    "                base = 3\n",
    "                keys = [4, 2, 5, 1, 6, 0]\n",
    "            elif method == 'cinl':\n",
    "                base = 3\n",
    "                keys = [0, 6, 1, 5, 2, 4]\n",
    "            elif method == 'cinr':\n",
    "                base = 3\n",
    "                keys = [6, 0, 5, 1, 4, 2]\n",
    "                \n",
    "        # Do the merging by looping through all indices and keys\n",
    "        for a_dir, images, homographies in self.load_images():\n",
    "            # Loop through the indices\n",
    "            for i in indices: \n",
    "                # Load the base image depending on the method\n",
    "                base_image_key = ''\n",
    "                if axis == 0:\n",
    "                    base_image_key = str(i) + base\n",
    "                elif axis == 1:\n",
    "                    base_image_key = str(base) + i\n",
    "                    \n",
    "                # Define the merged image and set it to the base image as the start\n",
    "                merged_image = images[base_image_key]\n",
    "                \n",
    "                if debug:\n",
    "                    print('directory: ', a_dir)\n",
    "                    print('base key: ', base_image_key)\n",
    "\n",
    "                for k in keys:\n",
    "                    # Get the key given the index and k\n",
    "                    key = ''\n",
    "                    if axis == 0:\n",
    "                        key = str(i) + k\n",
    "                    elif axis == 1:\n",
    "                         key = str(k) + i\n",
    "                        \n",
    "                    # Load the image and the corresponding homography matrix\n",
    "                    im = images[key]\n",
    "                    homography = np.array(homographies[key])\n",
    "                    \n",
    "                    # Warp the perspective (i.e. transform the current image into the perspective of the base image (-B01))\n",
    "                    im_warped = cv2.warpPerspective(im, homography, im.shape[:2])\n",
    "                    # Merge the images\n",
    "                    merged_image = cv2.addWeighted(merged_image, alpha, im_warped, 1 - alpha, 0.0)\n",
    "\n",
    "                self.merged_images[a_dir+str(i)] = merged_image\n",
    "                if debug:\n",
    "                    plt.imshow(merged_image)\n",
    "                    plt.show()\n",
    "            #break\n",
    "        return self.merged_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ddb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merger = ImageMerger('data_WiSAR/data/validation/', 'data_WiSAR/data/mask.png')\n",
    "valid_images = image_merger.merge_images(axis=0, method='coutl', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2222eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merger = ImageMerger('data_WiSAR/data/train/', 'data_WiSAR/data/mask.png')\n",
    "train_images = image_merger.merge_images(axis=0, method='coutl', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(image, threshold=4):\n",
    "\n",
    "    array = image.copy()\n",
    "    Xpix = array.shape[0]\n",
    "    Ypix = array.shape[1]\n",
    "    \n",
    "    # offset because of black regions:\n",
    "    xOffset = 110\n",
    "    yOffsetDown = 300\n",
    "    yOffsetUp = 1024 - 950\n",
    "    \n",
    "    mean_vector = np.mean(array, axis=(0, 1))\n",
    "    \n",
    "    reshaped_array = array.reshape((Xpix * Ypix, 3))\n",
    "\n",
    "    number=100000\n",
    "    # sample the above array to reduce computing time\n",
    "    choices = np.random.randint(0, len(reshaped_array), number)\n",
    "    reshaped_array = np.array([reshaped_array[i] for i in choices])\n",
    "    average = np.mean(reshaped_array)\n",
    "    # compute the variance-covariance matrix for these RGB data\n",
    "    matrix = np.array(sum([np.outer(np.array([reshaped_array[i] - \\\n",
    "       average]), np.array(reshaped_array[i] - average)) for i in \\\n",
    "        range(len(reshaped_array))]) / len(reshaped_array))\n",
    "\n",
    "    variance_covariance = np.linalg.inv(matrix)\n",
    "\n",
    "    # iterate over the original image and store dM in this new array\n",
    "    distances = np.zeros([Xpix, Ypix])\n",
    "    for i in range(yOffsetDown, Xpix-yOffsetUp):\n",
    "        for j in range(xOffset, Ypix):\n",
    "            distances[i][j] = np.sqrt(np.dot(np.dot(np.transpose(array[i][j] - mean_vector), variance_covariance),array[i][j] - mean_vector))\n",
    "\n",
    "    anomaly = np.zeros_like(distances)\n",
    "    purified = array.copy()\n",
    "    \n",
    "    for i in range(yOffsetDown, Xpix-yOffsetUp):\n",
    "        for j in range(xOffset, Ypix):\n",
    "            if distances[i,j] > threshold:\n",
    "                window = array[i-5:i+5, j-5: j+5]\n",
    "                unique, counts = np.unique(window.reshape(-1, 3), axis=0, return_counts=True)\n",
    "                anomaly[i,j] = 1\n",
    "                purified[i,j,0], purified[i,j,1], purified[i,j,2] = unique[np.argmax(counts)]\n",
    "            else:\n",
    "                anomaly[i,j] = 0\n",
    "        \n",
    "    return distances, anomaly, purified\n",
    "\n",
    "purified_train_images = []\n",
    "for i in range(7):\n",
    "    distances, anomaly, purified = mahalanobis(train_images['train-1-0'+str(i)], threshold=5)\n",
    "    purified_train_images.append(purified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.notebook import trange,tqdm\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    \"\"\" Convolutional auto-encoder. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 1, hid_channels: int = 64, code_channels: int = 256, \n",
    "                 kernel_size: int = 9, stride: int = 1, pooling: int = 2, \n",
    "                 activation: nn.Module = nn.LeakyReLU()):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of channels in the inputs.\n",
    "        hid_channels : int\n",
    "            Number of channels in the hidden layer(s).\n",
    "        code_channels : int\n",
    "            Number of channels in the code.\n",
    "        kernel_size : int or tuple\n",
    "            Window size for the convolutions\n",
    "        stride : int or tuple, optional\n",
    "            Window strides for the convolutions.\n",
    "        pooling : int or tuple, optional\n",
    "            Window size for the average pooling.\n",
    "        activation : nn.Module, optional\n",
    "            Activation function for the auto-encoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            activation,\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            activation,\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            activation\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, output_padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            activation,\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=1, output_padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            activation,\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=9, stride=1, output_padding=0),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "cae = ConvAutoEncoder()\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 9, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 5, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "    \n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 0\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# sanity check\n",
    "cae = ConvAutoEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "width = 256\n",
    "height = 256\n",
    "dim = (width, height)\n",
    "purified_train_images_resized = []\n",
    "for i in range(len(purified_train_images)):\n",
    "    im = cv2.resize(purified_train_images[i], dim, interpolation = cv2.INTER_AREA)\n",
    "    im = im[np.newaxis]\n",
    "    im = np.moveaxis(im, [0, 1, 2, 3], [0, 3, 2, 1])\n",
    "    im = torch.Tensor(im)\n",
    "    purified_train_images_resized.append(im)\n",
    "    \n",
    "y = cae(purified_train_images_resized[0])\n",
    "cae.encoder(purified_train_images_resized[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def _forward(network: nn.Module, data: DataLoader, metric: callable):\n",
    "    device = next(network.parameters()).device\n",
    "    \n",
    "    for x, y in data:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = network(x)\n",
    "        res = metric(logits, y)\n",
    "        yield res\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(network: nn.Module, data: DataLoader, metric: callable) -> list:\n",
    "    network.eval()\n",
    "    \n",
    "    results = _forward(network, data, metric)\n",
    "    return [res.item() for res in results]\n",
    "    \n",
    "\n",
    "@torch.enable_grad()\n",
    "def update(network: nn.Module, data: DataLoader, loss: nn.Module, \n",
    "           opt: optim.Optimizer) -> list:\n",
    "    network.train()\n",
    "    \n",
    "    errs = []\n",
    "    for err in _forward(network, data, loss):\n",
    "        errs.append(err.item())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return errs\n",
    "\n",
    "\n",
    "def train_auto_encoder(auto_encoder: nn.Module, loader: DataLoader, \n",
    "                       objective: nn.Module, optimiser: optim.Optimizer, \n",
    "                       num_epochs: int = 10, vis_every: int = 1):\n",
    "    \"\"\"\n",
    "    Train an auto-encoder for a number of epochs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    auto_encoder : nn.Module\n",
    "        The auto-encoder to train\n",
    "    loader : DataLoader\n",
    "        A data loader for iterating over batches of the data.\n",
    "    objective : nn.Module\n",
    "        The loss function to optimise during training.\n",
    "    optimiser : optim.Optimizer\n",
    "        The optimiser to use for training.\n",
    "    num_epochs : int, optional\n",
    "        Number of times to iterate the dataset.\n",
    "    vis_every : int, optional\n",
    "        Frequency, during training, of \n",
    "        intermediate visualisation of reconstructions.\n",
    "    \"\"\"\n",
    "    # take random batch for visualising reconstructions\n",
    "    ref_inputs, _ = next(iter(loader))\n",
    "    \n",
    "    # evaluate random performance\n",
    "    errs = evaluate(auto_encoder, loader, objective)\n",
    "    print(f\"Epoch {0: 2d} - avg loss: {sum(errs) / len(errs):.6f}\")\n",
    "    display_result(auto_encoder, ref_inputs)\n",
    "    \n",
    "    # train for some epochs\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        errs = update(auto_encoder, loader, objective, optimiser)\n",
    "        print(f\"Epoch {epoch: 2d} - avg loss: {sum(errs) / len(errs):.6f}\")\n",
    "\n",
    "        if epoch % vis_every == 0:\n",
    "            display_result(auto_encoder, ref_inputs)\n",
    "            \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import trange,tqdm\n",
    "import gc\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 1\n",
    "\n",
    "#dataroot ='data_WiSAR/data/train'\n",
    "#dataset = dset.ImageFolder(root=dataroot,\n",
    "#                           transform=transforms.Compose([\n",
    "#                               transforms.Resize(image_size),\n",
    "#                               transforms.CenterCrop(image_size),\n",
    "#                               transforms.ToTensor(),\n",
    "#                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#                           ]))\n",
    "dataset = purified_train_images_resized\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(cae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for batch_features in dataloader:\n",
    "        im = torch.squeeze(batch_features, dim=0)\n",
    "        im = nn.functional.normalize(im)\n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = cae(im)\n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, im)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimiser.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(dataloader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, num_epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "  cae, \n",
    "  'cae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cae = torch.load('cae.pth', map_location='cpu')\n",
    "cae.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45170883",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "dim = (width, height)\n",
    "\n",
    "test_image = train_images['train-2-10']    \n",
    "test_image = cv2.resize(test_image, dim, interpolation = cv2.INTER_AREA)\n",
    "test_image = test_image[np.newaxis]\n",
    "test_image = np.moveaxis(test_image, [0, 1, 2, 3], [0, 3, 2, 1])\n",
    "test_image = torch.Tensor(test_image)\n",
    "    \n",
    "reconstructed = cae.forward(nn.functional.normalize(purified_train_images_resized[0]))\n",
    "reconstructed = torch.squeeze(reconstructed.detach()).T\n",
    "width = 1024\n",
    "height = 1024\n",
    "dim = (width, height)\n",
    "reconstructed = cv2.resize(reconstructed.cpu().detach().numpy(), dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "plt.figure(figsize=(10.6, 10.6))\n",
    "plt.imshow((reconstructed * 255).astype(np.uint8))\n",
    "\n",
    "reconstructed = cae.forward(nn.functional.normalize(test_image))\n",
    "reconstructed = torch.squeeze(reconstructed.detach()).T\n",
    "width = 1024\n",
    "height = 1024\n",
    "dim = (width, height)\n",
    "reconstructed = cv2.resize(reconstructed.cpu().detach().numpy(), dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "plt.figure(figsize=(10.6, 10.6))\n",
    "plt.imshow((reconstructed * 255).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
