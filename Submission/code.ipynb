{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6bbe2c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587884d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "import cv2\n",
    "import imutils\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import skimage\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cddfa6",
   "metadata": {},
   "source": [
    "The evaluation script was copied in rather than imported. Please note that we left the code as it was and made NO adjustments to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fc54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, NewType, List, Tuple, Union\n",
    "import pathlib\n",
    "\n",
    "Path = Union[pathlib.Path, str]\n",
    "BoundingBox = NewType(\"BoundingBox\", Tuple[int, int, int, int])\n",
    "YoloBox = NewType(\"YoloBox\", Tuple[float, float, float, float])\n",
    "Shape = NewType(\"Shape\", Tuple[int, int])\n",
    "\n",
    "\n",
    "def compute_AP(detections: Dict[str, List[BoundingBox]],\n",
    "               targets: Dict[str, List[BoundingBox]]) -> float:\n",
    "    \"\"\" Compute the average precision.\n",
    "    Params:\n",
    "        detections: list of detected bounding boxes within each sample\n",
    "        targets: list of ground truth bounding boxes within each sample\n",
    "    \"\"\"\n",
    "    # define the IoU threshold sequence\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "    precision = np.zeros_like(thresholds)\n",
    "    recall = np.zeros_like(thresholds)\n",
    "\n",
    "    iou_scores = [compute_IoU(detections[k], targets[k])\n",
    "                  for k in targets.keys()]\n",
    "\n",
    "    for i, iou_th in enumerate(thresholds):\n",
    "        true_positives = sum(\n",
    "            [np.sum(np.any(iou > iou_th, 1)) for iou in iou_scores])\n",
    "        false_positives = sum(\n",
    "            [np.sum(~np.any(iou > iou_th, 1)) for iou in iou_scores])\n",
    "        false_negatives = sum(\n",
    "            [np.sum(~np.any(iou > iou_th, 0)) for iou in iou_scores])\n",
    "        \n",
    "        if true_positives + false_positives:\n",
    "            precision[i] = true_positives/(true_positives+false_positives)\n",
    "        else:\n",
    "            precision[i] = 0\n",
    "        recall[i] = true_positives/(true_positives+false_negatives)\n",
    "\n",
    "    # compute average precision\n",
    "    recall = np.append(recall, 0)\n",
    "    ap = np.sum((recall[:-1] - recall[1:]) * precision)\n",
    "    return ap\n",
    "\n",
    "\n",
    "def compute_IoU(detections: List[BoundingBox],\n",
    "                targets: List[BoundingBox]) -> np.array:\n",
    "    \"\"\" Compute the intersection of union (IoU) score.\n",
    "    Params:\n",
    "        detections: detected bounding boxes\n",
    "        targets: ground truth bounding boxes\n",
    "    Return:\n",
    "        Array of IoU score between each pair of detected and target bounding\n",
    "        box, where the detections are along the rows and the targets along\n",
    "        the columns.\n",
    "    \"\"\"\n",
    "    iou = np.empty((len(detections), len(targets)))\n",
    "\n",
    "    for i, d in enumerate(detections):\n",
    "        dx, dy, dw, dh = d\n",
    "        for j, t in enumerate(targets):\n",
    "            tx, ty, tw, th = t\n",
    "            x = max(dx, tx)\n",
    "            y = max(dy, ty)\n",
    "            xx = min(dx + dw, tx + tw)\n",
    "            yy = min(dy + dh, ty + th)\n",
    "            intersection_area = max(0, xx-x) * max(0, yy-y)\n",
    "            iou[i, j] = intersection_area / (dw*dh + tw*th - intersection_area)\n",
    "                        \n",
    "    return iou\n",
    "\n",
    "\n",
    "def read_bb(file: Path) -> Dict[str, List[BoundingBox]]:\n",
    "    \"\"\" Read bounding boxes from json file.\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        js = json.load(f)\n",
    "    return js\n",
    "\n",
    "\n",
    "def write_bb(file: Path, bbs: Dict[str, List[BoundingBox]]) -> None:\n",
    "    \"\"\" Write bounding boxes to json file.\n",
    "    \"\"\"\n",
    "    with open(file, \"w\") as f:\n",
    "        json.dump(bbs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70945f",
   "metadata": {},
   "source": [
    "# Image merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f6f84",
   "metadata": {},
   "source": [
    "### Merging hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cfe1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.75\n",
    "method = 'outl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33625cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMerger:\n",
    "    \"\"\"\n",
    "    ImageMerger can merge images from a dataset including images collected by a drone with the purpose of detecting \n",
    "    moving people in the images.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, mask_file):\n",
    "        \"\"\"\n",
    "        :param data_dir: path to the directory where the data is located\n",
    "        :param mask_file: path of a mask which removes unneccesary text from the images\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get all directories inside the data_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.dirs = os.listdir(data_dir)\n",
    "        self.mask = np.array(cv2.imread(mask_file)) // 255\n",
    "        \n",
    "        # Store the merged images\n",
    "        self.merged_images = {}\n",
    "        \n",
    "    def load_images(self):\n",
    "        \"\"\"\n",
    "        Loads the images and homographies from one directory at a time and yields them\n",
    "        \"\"\"\n",
    "        \n",
    "        for a_dir in self.dirs:\n",
    "            if a_dir == 'labels.json':\n",
    "                continue\n",
    "            input_images = {}\n",
    "            \n",
    "            # Load all images in the train directory\n",
    "            image_dir_file_path = os.path.join(os.path.join(self.data_dir, a_dir), '*.png')\n",
    "            images = glob.glob(image_dir_file_path)\n",
    "            \n",
    "            # Copy all the raw images to the input images dict\n",
    "            for im in images:\n",
    "                file = os.path.basename(im)\n",
    "                img_name = os.path.splitext(file)[0]\n",
    "                src = cv2.cvtColor(cv2.imread(im), cv2.COLOR_BGR2RGB)\n",
    "                src = self.apply_mask(src)\n",
    "                input_images[img_name] = src\n",
    "            \n",
    "            # Load the homographies json file\n",
    "            homographies_file_path = os.path.join(os.path.join(self.data_dir, a_dir), 'homographies.json')\n",
    "            with open(homographies_file_path, 'rb') as f:\n",
    "                homographies = json.load(f)\n",
    "            \n",
    "            yield a_dir, input_images, homographies\n",
    "        \n",
    "    def apply_mask(self, image):\n",
    "        \"\"\"\n",
    "        Applys the provided mask on image\n",
    "        :param image: image file which will be treated as np.array\n",
    "        \"\"\"\n",
    "\n",
    "        return image * self.mask\n",
    "    \n",
    "    def merge_images(self, alpha=0.75, axis=0, method='ltr', debug=False):\n",
    "        \"\"\"\n",
    "        Merges all the images given the homographies which are retrieved from load_data. \n",
    "        :param alpha: Weighting for merging the images. The image to be merged on will be considered with a weight a alpha,\n",
    "        the second image with a weight of 1-alpha.\n",
    "        :param axis: defines the axis along which the images shall be merged. 0: camera axis, 1: time axis\n",
    "        :param method: defines in which order the images shall be merged:\n",
    "            - ltr:  The images are merged from left to right along the camera axis or from the first in time (index 0) \n",
    "                    to the last in time (index 6)\n",
    "            - rtl:  The images are merged from right to left along the camera axis or from the last in time (index 6) \n",
    "                    to the first in time (index 0)\n",
    "            - outl: The images are merged from the center out switching between the left and right side of the center \n",
    "                    (along camera axis)/previous and next in time (along time axis) starting with the left side/previous \n",
    "                    in time. \n",
    "            - outr: The images are merged from the center out switching between the left and right side of the center \n",
    "                    (along camera axis)/previous and next in time (along time axis) starting with the right side/next in \n",
    "                    time.\n",
    "            - inl:  The images are merged from the outside inward switching between the left and right side of the center \n",
    "                    (along camera axis)/previous and next in time (along time axis) starting with the left side/previous \n",
    "                    in time. \n",
    "            - inr:  The images are merged from the outside inward switching between the left and right side of the center \n",
    "                    (along camera axis)/previous and next in time (along time axis) starting with the right side/next in \n",
    "                    time.\n",
    "        :param debug: if True, results are printed to be able to debug \n",
    "        \"\"\"\n",
    "        \n",
    "        assert method in ['ltr', 'rtl', 'outl', 'outr', 'inl', 'inr']\n",
    "        \n",
    "        # Define the keys and indices depending on axis and method\n",
    "        keys = []\n",
    "        indices = []\n",
    "        if axis == 0:\n",
    "            indices = range(0, 7)\n",
    "            if method == 'ltr':\n",
    "                keys = ['-B05', '-B04', '-B03', '-B02', '-B01', '-G01', '-G02', '-G03', '-G04', '-G05']\n",
    "            elif method == 'rtl': \n",
    "                keys = ['-G05', '-G04', '-G03', '-G02', '-G01', '-B01', '-B02', '-B03', '-B04', '-B05']\n",
    "            elif method == 'outl':\n",
    "                keys = ['-B01', '-G01', '-B02', '-G02', '-B03', '-G03', '-B04', '-G04', '-B05', '-G05']\n",
    "            elif method == 'outr':\n",
    "                keys = ['-G01', '-B01', '-G02', '-B02', '-G03', '-B03', '-G04', '-B04', '-G05', '-B05']\n",
    "            elif method == 'inl':\n",
    "                keys = ['-B05', '-G05', '-B04', '-G04', '-B03', '-G03', '-B02', '-G02', '-B01', '-G01']\n",
    "            elif method == 'inr':\n",
    "                keys = ['-G05', '-B05', '-G04', '-B04', '-G03', '-B03', '-G02', '-B02', '-G01', '-B01']\n",
    "        elif axis == 1:\n",
    "            indices = ['-B05', '-B04', '-B03', '-B02', '-B01', '-G01', '-G02', '-G03', '-G04', '-G05']\n",
    "            if method == 'ltr':\n",
    "                keys = [0, 1, 2, 3, 4, 5, 6]\n",
    "            elif method == 'rtl': \n",
    "                keys = [6, 5, 4, 3, 2, 1, 0]\n",
    "            elif method == 'outl':\n",
    "                keys = [3, 2, 4, 1, 5, 0, 6]\n",
    "            elif method == 'outr':\n",
    "                keys = [3, 4, 2, 5, 1, 6, 0]\n",
    "            elif method == 'inl':\n",
    "                keys = [0, 6, 1, 5, 2, 4, 3]\n",
    "            elif method == 'inr':\n",
    "                keys = [6, 0, 5, 1, 4, 2, 3]\n",
    "                \n",
    "        # Do the merging by looping through all indices and keys\n",
    "        for a_dir, images, homographies in self.load_images():\n",
    "            # Loop through the indices\n",
    "            for i in indices: \n",
    "                # Load the base image depending on the method\n",
    "                base_image_key = ''\n",
    "                if axis == 0:\n",
    "                    base_image_key = str(i) + keys[0]\n",
    "                elif axis == 1:\n",
    "                    base_image_key = str(keys[0]) + i\n",
    "                    \n",
    "                # Define the merged image and set it to the base image as the start\n",
    "                merged_image = images[base_image_key]\n",
    "                \n",
    "                if debug:\n",
    "                    print('directory: ', a_dir)\n",
    "                    print('base key: ', base_image_key)\n",
    "\n",
    "                for k in keys:\n",
    "                    # Get the key given the index and k\n",
    "                    key = ''\n",
    "                    if axis == 0:\n",
    "                        key = str(i) + k\n",
    "                    elif axis == 1:\n",
    "                         key = str(k) + i\n",
    "                        \n",
    "                    # Load the image and the corresponding homography matrix\n",
    "                    im = images[key]\n",
    "                    homography = np.array(homographies[key])\n",
    "                    \n",
    "                    # Warp the perspective (i.e. transform the current image into the perspective of the base image (-B01))\n",
    "                    im_warped = cv2.warpPerspective(im, homography, im.shape[:2])\n",
    "                    # Merge the images\n",
    "                    merged_image = cv2.addWeighted(merged_image, alpha, im_warped, 1 - alpha, 0.0)\n",
    "\n",
    "                self.merged_images[a_dir + '-' + str(i)] = merged_image\n",
    "                \n",
    "                if debug:\n",
    "                    plt.imshow(merged_image)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31ddb76",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: '../4_Notebooks/data_WiSAR/data/validation/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14816/4169189804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_merger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageMerger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../4_Notebooks/data_WiSAR/data/validation/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../4_Notebooks/data_WiSAR/data/mask.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage_merger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvalid_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_merger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerged_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14816/1830053081.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_dir, mask_file)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Get all directories inside the data_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: '../4_Notebooks/data_WiSAR/data/validation/'"
     ]
    }
   ],
   "source": [
    "image_merger = ImageMerger('../4_Notebooks/data_WiSAR/data/validation/', '../4_Notebooks/data_WiSAR/data/mask.png')\n",
    "image_merger.merge_images(axis=0, method=method, debug=False, alpha=alpha)\n",
    "valid_images = image_merger.merged_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2222eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merger = ImageMerger('../4_Notebooks/data_WiSAR/data/test/', '../4_Notebooks/data_WiSAR/data/mask.png')\n",
    "image_merger.merge_images(axis=0, method=method, debug=False, alpha=alpha)\n",
    "test_images = image_merger.merged_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cf7f2",
   "metadata": {},
   "source": [
    "# Import of ground truth of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "with open(os.path.join('../4_Notebooks/data_WiSAR/data/validation/', 'labels.json'), 'r') as file:\n",
    "    ground_truth = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ede51",
   "metadata": {},
   "source": [
    "# Approach leading to best precision on validation set\n",
    "\n",
    "As described in the method pdf, for the approach that led to the best precision on the validation set, we use the first, center and last time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba42768",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_area = 3\n",
    "threshold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f579530",
   "metadata": {},
   "source": [
    "### Anomaly images\n",
    "The anomaly images are obtained using the Mahalanobis distance and thresholding the distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fa00b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mahalanobis(image, threshold=5):\n",
    "    array = image.copy()\n",
    "    mean_vector = np.mean(array, axis=(0, 1))  \n",
    "    \n",
    "    im_re = array.reshape(-1, 3).astype(np.float64)\n",
    "    im_re -= im_re.mean(0, keepdims=True)\n",
    "    im_re_cov = 1/(im_re.shape[0]-1) * im_re.T @ im_re\n",
    "        \n",
    "    cov = np.linalg.inv(im_re_cov)\n",
    "            \n",
    "    am = array - mean_vector\n",
    "    amc = am @ cov\n",
    "    distances = np.sqrt(np.einsum('ijk,ijk->ij', amc, am))\n",
    "    \n",
    "    anomaly = np.zeros_like(distances)   \n",
    "    anomaly[distances > threshold] = 1\n",
    "        \n",
    "    return distances, anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38883f49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clear_anomaly(images, key, show_images=False, threshold=5):\n",
    "    image_0 = images[key + '-0']\n",
    "    distances_0, anomaly_0 = mahalanobis(image_0, threshold=threshold)\n",
    "\n",
    "    image_6 = images[key + '-6']\n",
    "    distances_6, anomaly_6 = mahalanobis(image_6, threshold=threshold)\n",
    "    \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(image_0)\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(anomaly_0)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(image_6)\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(anomaly_6)\n",
    "        plt.show()\n",
    "\n",
    "    both = anomaly_0 * anomaly_6\n",
    "\n",
    "    image_3 = images[key + '-3']\n",
    "    distances_3, anomaly_3 = mahalanobis(image_3, threshold=threshold)\n",
    "        \n",
    "    if show_images:\n",
    "        \n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(image_3)\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(anomaly_3)\n",
    "        plt.show()\n",
    "\n",
    "    contours_both, _ = cv2.findContours(both.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    contours_image_3, _ = cv2.findContours(anomaly_3.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Remove tiny areas\n",
    "    remaining_cnts = []\n",
    "    for cnt in contours_image_3:\n",
    "        area = cv2.contourArea(cnt)\n",
    "\n",
    "        if area <= 10:\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            anomaly_3[y:y+h, x:x+w] = 0\n",
    "        else:\n",
    "            remaining_cnts.append(cnt)\n",
    "\n",
    "    # Get close contours\n",
    "    min_distance = 20\n",
    "    close_contours = []\n",
    "    for i in range(len(remaining_cnts) - 1):\n",
    "        for j in range(i + 1, len(remaining_cnts)):\n",
    "            for pt in remaining_cnts[j]:\n",
    "                r_cnt = remaining_cnts[i].reshape(-1, 2)\n",
    "                diff = r_cnt - pt\n",
    "                dist = np.sqrt(diff[:, 0] ** 2 + diff[:, 1] ** 2)\n",
    "                if dist.min() < min_distance:\n",
    "                    close_contours.append((i, j))\n",
    "                    break\n",
    "        \n",
    "    def remove_close_cnts(i):\n",
    "        for j, (m, n) in enumerate(close_contours):\n",
    "            if i == m:\n",
    "                cv2.drawContours(mask, [remaining_cnts[n]], -1, 0, -1)\n",
    "                close_contours.remove(close_contours[j])\n",
    "                remove_close_cnts(n)\n",
    "            elif i == n:\n",
    "                cv2.drawContours(mask, [remaining_cnts[m]], -1, 0, -1)\n",
    "                close_contours.remove(close_contours[j])\n",
    "                remove_close_cnts(m)\n",
    "\n",
    "    # Remove all contours that seem to be static and create a mask\n",
    "    mask = np.ones_like(anomaly_3)\n",
    "    i = 0\n",
    "    for i, r_cnt in enumerate(remaining_cnts):\n",
    "        for cnt in contours_both:\n",
    "            cnt = cnt.reshape(-1, 2)\n",
    "            for x, y in cnt:\n",
    "                if cv2.pointPolygonTest(r_cnt, (float(x), float(y)), True) >= 0:\n",
    "                    cv2.drawContours(mask, [r_cnt], -1, 0, -1)\n",
    "                    remove_close_cnts(i)\n",
    "                    break\n",
    "    \n",
    "    # Return the masked anomaly image of timestep 3\n",
    "    return anomaly_3 * mask\n",
    "\n",
    "def draw_bounding_boxes(anomaly_image, images, key, show_images=False, max_area=3, add_ground_truth=False):\n",
    "    image = images[key + '-3'].copy()\n",
    "    kernel = np.ones((10, 10),np.uint8)\n",
    "    anomaly_image = cv2.morphologyEx(anomaly_image.astype('uint8'), cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(anomaly_image.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    bb_vals = []\n",
    "        \n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area >= 20:\n",
    "            uncertainty = min(int(max(750 / area, 1)), max_area)\n",
    "            \n",
    "            rect = cv2.boundingRect(cnt)\n",
    "            x,y,w,h = rect\n",
    "            #print(x, y, w, h)\n",
    "            w, h = w * uncertainty, h * uncertainty\n",
    "            x, y = x - w // 2, y - h // 2 \n",
    "            #print(x, y, w, h)\n",
    "            \n",
    "            roi = images[key + '-3'][y:y+h, x:x+w]\n",
    "            \n",
    "            #plt.figure(figsize=(10.6, 10.6))\n",
    "            #plt.imshow(roi)\n",
    "            #plt.show()\n",
    "            \n",
    "            \"\"\"\n",
    "            print(mean, std)\n",
    "            print(mean + std)\n",
    "            print(roi.mean(axis=(0, 1)), roi.std(axis=(0, 1)))\n",
    "            print(roi.mean(axis=(0, 1)) > mean + std)\n",
    "            print(roi.mean(axis=(0, 1)) < mean - std)\n",
    "            print(roi.mean(axis=(0, 1)) + roi.std(axis=(0, 1)) > mean + std)\n",
    "            print(roi.mean(axis=(0, 1)) - roi.std(axis=(0, 1)) < mean - std)\"\"\"\n",
    "            \n",
    "            m = roi.mean(axis=(0, 1))\n",
    "            s = roi.std(axis=(0, 1))\n",
    "            \n",
    "            \n",
    "            if (m + s > mean + std).all() or (m - s < mean - std).all():\n",
    "                bb_vals.append([x, y, w, h])\n",
    "                if show_images:\n",
    "                    image = cv2.rectangle(image, (x,y), (x+w,y+h), (0, 255, 0), 2)\n",
    "            elif np.sum(m + s > mean + std) == 2:\n",
    "                tolerance = 1\n",
    "                if (m + s + tolerance > mean + std).all(): \n",
    "                    bb_vals.append([x, y, w, h])\n",
    "                            \n",
    "                    if show_images:\n",
    "                        image = cv2.rectangle(image, (x,y), (x+w,y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        \n",
    "        if add_ground_truth and (key + '-3') in valid_images:\n",
    "            for box in ground_truth[key]:\n",
    "                start_point = (box[0], box[1])\n",
    "                end_point = (box[0] + box[3], box[1] + box[2])\n",
    "\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "                thickness = 2\n",
    "                image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "                \n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "    return bb_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_images(images):\n",
    "    keys = images.keys()\n",
    "    means = np.zeros(shape=(len(keys), 3))   \n",
    "    stds = np.zeros_like(means)\n",
    "\n",
    "    for i, key in enumerate(keys): \n",
    "        image = images[key]\n",
    "        means[i] = np.mean(image, axis=(0, 1))\n",
    "        stds[i] = np.std(image, axis=(0, 1))\n",
    "\n",
    "    mean = means.mean(axis=0)\n",
    "    std = stds.mean(axis=0)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "    print('Mean: ', mean, ', Standard deviation: ', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f015a69",
   "metadata": {},
   "source": [
    "### Predictions on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841a2f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_strings = {0: \"valid-1-0\", 1: \"valid-1-1\", 2: \"valid-1-2\", 3: \"valid-1-3\", 4: \"valid-1-4\", 5: \"valid-1-5\",\n",
    "                 6: \"valid-1-6\", 7: \"valid-2-0\", 8: \"valid-2-1\", 9: \"valid-2-2\", 10: \"valid-2-3\"}\n",
    "\n",
    "bb_pred_dict = {}\n",
    "\n",
    "mean, std = analyze_images(valid_images)\n",
    "\n",
    "for j in valid_strings:\n",
    "    print(valid_strings[j])\n",
    "    cleared_image = clear_anomaly(valid_images, valid_strings[j], show_images=False, threshold=threshold)\n",
    "    bb_pred_dict[valid_strings[j]] = draw_bounding_boxes(cleared_image, valid_images, valid_strings[j], show_images=True, add_ground_truth=True)\n",
    "\n",
    "result = str(bb_pred_dict)\n",
    "result = result.replace(\"'\", \"\\\"\")\n",
    "print(result)\n",
    "with open(\"val.json\",\"w\") as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa113a",
   "metadata": {},
   "source": [
    "### Precision on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397728f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "def evaluate(detections: Dict[str, List[BoundingBox]],\n",
    "             targets: Dict[str, List[BoundingBox]]) -> float:\n",
    "    return compute_AP(detections, targets)\n",
    "\n",
    "\n",
    "# load the detections\n",
    "detections = read_bb(\"val.json\")\n",
    "# load the targets\n",
    "targets = read_bb(os.path.join('../4_Notebooks/data_WiSAR/data/validation/', 'labels.json'))\n",
    "\n",
    "ap = evaluate(detections, targets)\n",
    "print(f\"Average precision={ap:.5f} on validation set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d6357",
   "metadata": {},
   "source": [
    "### Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e57ee7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_strings = {0: \"test-1-7\", 1: \"test-1-8\", 2: \"test-1-9\", 3: \"test-1-10\", 4: \"test-1-11\", 5: \"test-1-12\",\n",
    "                 6: \"test-1-13\", 7: \"test-1-14\", 8: \"test-2-4\", 9: \"test-2-5\", 10: \"test-2-6\", 11: \"test-2-7\", \n",
    "                 12: \"test-2-8\"}\n",
    "\n",
    "bb_pred_dict = {}\n",
    "\n",
    "mean, std = analyze_images(test_images)\n",
    "\n",
    "for j in test_strings:\n",
    "    print(test_strings[j])\n",
    "    cleared_image = clear_anomaly(test_images, test_strings[j], show_images=False, threshold=threshold)\n",
    "    bb_pred_dict[test_strings[j]] = draw_bounding_boxes(cleared_image, test_images, test_strings[j], show_images=True, add_ground_truth=True)\n",
    "\n",
    "result = str(bb_pred_dict)\n",
    "result = result.replace(\"'\", \"\\\"\")\n",
    "print(result)\n",
    "with open(\"test.json\",\"w\") as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeff271",
   "metadata": {},
   "source": [
    "# What we tried but didn't improve results (code added for reference)\n",
    "\n",
    "We tried to include all timesteps to find out which objects (anomalies) are static and which not. Then we removed the static anomalies from every anomaly image of every timestep and added those images up. This would help to detect also people that are not visible in the center timestep but in others. We did pretty well on detecting people but also detected objects as people that aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(image, threshold=5):\n",
    "    array = image.copy()\n",
    "    mean_vector = np.mean(array, axis=(0, 1))  \n",
    "    \n",
    "    im_re = array.reshape(-1, 3).astype(np.float64)\n",
    "    im_re -= im_re.mean(0, keepdims=True)\n",
    "    im_re_cov = 1/(im_re.shape[0]-1) * im_re.T @ im_re\n",
    "        \n",
    "    cov = np.linalg.inv(im_re_cov)\n",
    "            \n",
    "    am = array - mean_vector\n",
    "    amc = am @ cov\n",
    "    distances = np.sqrt(np.einsum('ijk,ijk->ij', amc, am))\n",
    "        \n",
    "    distances = distances/distances.max()\n",
    "    threshold = np.quantile(distances, .999)\n",
    "    \n",
    "    anomaly = np.zeros_like(distances)   \n",
    "    anomaly[distances > threshold] = 1\n",
    "        \n",
    "    return distances, anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    y_nonzero, x_nonzero, _ = np.nonzero(image)\n",
    "    return image[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_anomaly_new(images, key, show_images=False, threshold=threshold):\n",
    "    anomaly_ims = []\n",
    "    \n",
    "    image_0 = images[key + '-0'].copy()\n",
    "    image_0_org = image_0.copy()\n",
    "    \n",
    "    image_0 = reduce_image(image_0)\n",
    "\n",
    "    _, anomaly_0 = mahalanobis(image_0)\n",
    "    anomaly_0 = remove_wrong_anomalies(anomaly_0, image_0_org)\n",
    "    base = anomaly_0\n",
    "    \n",
    "    anomaly_ims.append(anomaly_0)\n",
    "    \n",
    "    # Take care of moving elements that move just a little by removing only those elements which area doesn't change\n",
    "    # much in the multiplied image\n",
    "    areas_0 = get_anomaly_0_areas(base)\n",
    "    \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(anomaly_0)\n",
    "        plt.show()\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        image = images[key + '-' + str(i)].copy()\n",
    "        image_org = image.copy()\n",
    "        \n",
    "        image = reduce_image(image)\n",
    "        \n",
    "        _, anomaly = mahalanobis(image, threshold=threshold)\n",
    "        anomaly = remove_wrong_anomalies(anomaly, image_org)\n",
    "        \n",
    "        anomaly_ims.append(anomaly)\n",
    "\n",
    "        base = anomaly * base\n",
    "        \n",
    "        # Remove stored areas if they don't appear in base anymore\n",
    "        base_contours, _ = cv2.findContours(base.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        if len(base_contours) < len(areas_0):\n",
    "            anomaly_0_non_static = remove_static_anomalies(anomaly_0, base, None)\n",
    "            anomaly_0_remaining = 1 - anomaly_0_non_static\n",
    "            \n",
    "            anomaly_0 = anomaly_0 * anomaly_0_remaining\n",
    "            areas_0 = get_anomaly_0_areas(anomaly_0)\n",
    "        \n",
    "        if show_images:\n",
    "            print(i)\n",
    "            plt.figure(figsize=(10.6, 10.6))\n",
    "            plt.imshow(base)\n",
    "            plt.show()\n",
    "    \n",
    "    # Check area decline of contours in multiplied image. If area decreased a lot relatively, it is likely moving\n",
    "    contours, _ = cv2.findContours(base.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    for i, cnt in enumerate(contours):\n",
    "        area = cv2.contourArea(cnt)\n",
    "        \n",
    "        if len(contours) <= len(areas_0):\n",
    "            if areas_0[i] != 0 and area/areas_0[i] < 0.05:\n",
    "                rect = cv2.boundingRect(cnt)\n",
    "                x,y,w,h = rect\n",
    "                base[y:y+h, x:x+w] = 0\n",
    "                \n",
    "        if area < 10:\n",
    "            rect = cv2.boundingRect(cnt)\n",
    "            x,y,w,h = rect\n",
    "            base[y:y+h, x:x+w] = 0\n",
    "\n",
    "    masked_0 = remove_static_anomalies(anomaly_ims[0], base, image_0)\n",
    "    \n",
    "    full_masked = masked_0\n",
    "    \n",
    "    for anomaly in anomaly_ims:\n",
    "        masked = remove_static_anomalies(anomaly, base, None)\n",
    "        full_masked += masked\n",
    "        \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(full_masked)\n",
    "        plt.show()\n",
    "    \n",
    "    contours, _ = cv2.findContours(full_masked.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    cnts_anoms = [False] * len(contours)\n",
    "    \n",
    "    for i in range(7):\n",
    "        image = images[key + '-' + str(i)]\n",
    "        mean = image.mean(axis=(0, 1))\n",
    "        std = image.std(axis=(0, 1))\n",
    "        \n",
    "        for j, cnt in enumerate(contours):\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            if area_fo_statistics(image[y:y+h, x:x+w], mean, std, show_images=False): \n",
    "                cnts_anoms[j] = True\n",
    "    \n",
    "    for cnt_anom, cnt in zip(cnts_anoms, contours):\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        \n",
    "        if not cnt_anom:\n",
    "            full_masked[y:y+h, x:x+w] = 0\n",
    "    \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(full_masked)\n",
    "        plt.show()\n",
    "        \n",
    "    contours, _ = cv2.findContours(full_masked.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        \n",
    "        if area < 5:\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            full_masked[y:y+h, x:x+w] = 0\n",
    "\n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(full_masked)\n",
    "        plt.show()\n",
    "    \n",
    "    return full_masked\n",
    "\n",
    "def reduce_image(image):\n",
    "    im = image.copy()\n",
    "    \n",
    "    mean = crop(im).mean(axis=(0,1))\n",
    "    std = crop(im).std(axis=(0,1))\n",
    "\n",
    "    im[im[:, :, 0] < mean[0] + std[0]] = 0\n",
    "    im[im[:, :, 1] < mean[1] + std[1]] = 0\n",
    "    im[im[:, :, 2] < mean[2] + std[2]] = 0\n",
    "\n",
    "    q95 = np.quantile(im, 0.95, axis=(0,1))\n",
    "    q99 = np.quantile(im, 0.99, axis=(0,1))\n",
    "\n",
    "    im[im[:, :, 0] < q95[0]] = 0\n",
    "    im[im[:, :, 1] > q99[1]] = 0\n",
    "    im[im[:, :, 2] < q95[2]] = 0\n",
    "    \n",
    "    return im\n",
    "    \n",
    "def get_anomaly_0_areas(anomaly_0):\n",
    "    contours, _ = cv2.findContours(anomaly_0.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    areas_0 = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        areas_0.append(area)\n",
    "    \n",
    "    return areas_0\n",
    "\n",
    "def draw_bounding_boxes_new(anomaly_image, images, key, show_images=False, max_area=3, add_ground_truth=False):\n",
    "    image = images[key + '-3'].copy()\n",
    "    kernel = np.ones((10, 10),np.uint8)\n",
    "    anomaly_image = cv2.morphologyEx(anomaly_image.astype('uint8'), cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(anomaly_image.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    bb_vals = []\n",
    "        \n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area >= 5:\n",
    "            uncertainty = min(int(max(800 / area, 1)), max_area)\n",
    "            \n",
    "            rect = cv2.boundingRect(cnt)\n",
    "            x,y,w,h = rect\n",
    "            w, h = w * uncertainty, h * uncertainty\n",
    "            x, y = x - w // 2, y - h // 2\n",
    "            \n",
    "            bb_vals.append([x, y, w, h])\n",
    "            \n",
    "            if show_images:\n",
    "                image = cv2.rectangle(image, (x,y), (x+w,y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        \n",
    "        if add_ground_truth and (key + '-3') in valid_images:\n",
    "            for box in ground_truth[key]:\n",
    "                start_point = (box[0], box[1])\n",
    "                end_point = (box[0] + box[3], box[1] + box[2])\n",
    "\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "                thickness = 2\n",
    "                image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "                \n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "    return bb_vals\n",
    "\n",
    "def remove_static_anomalies(anomaly_image, static_anomalies_image, original_image, show_images=False):\n",
    "    contours_static, _ = cv2.findContours(static_anomalies_image.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    contours_image, _ = cv2.findContours(anomaly_image.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    # Remove tiny areas\n",
    "    remaining_cnts = []\n",
    "    for cnt in contours_image:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        \n",
    "        if area <= 10:\n",
    "            anomaly_image[y:y+h, x:x+w] = 0\n",
    "        else:\n",
    "            remaining_cnts.append(cnt)\n",
    "\n",
    "    # Get close contours\n",
    "    min_distance = 10\n",
    "    close_contours = []\n",
    "    for i in range(len(remaining_cnts) - 1):\n",
    "        for j in range(i + 1, len(remaining_cnts)):\n",
    "            for pt in remaining_cnts[j]:\n",
    "                r_cnt = remaining_cnts[i].reshape(-1, 2)\n",
    "                diff = r_cnt - pt\n",
    "                dist = np.sqrt(diff[:, 0] ** 2 + diff[:, 1] ** 2)\n",
    "                if dist.min() < min_distance:\n",
    "                    close_contours.append((i, j))\n",
    "                    break\n",
    "        \n",
    "    def remove_close_cnts(i):\n",
    "        for j, (m, n) in enumerate(close_contours):\n",
    "            if i == m:\n",
    "                cv2.drawContours(mask, [remaining_cnts[n]], -1, 0, -1)\n",
    "                close_contours.remove(close_contours[j])\n",
    "                remove_close_cnts(n)\n",
    "            elif i == n:\n",
    "                cv2.drawContours(mask, [remaining_cnts[m]], -1, 0, -1)\n",
    "                close_contours.remove(close_contours[j])\n",
    "                remove_close_cnts(m)\n",
    "\n",
    "    # Remove all contours that seem to be static\n",
    "    mask = np.ones_like(anomaly_image)\n",
    "    i = 0\n",
    "    for i, r_cnt in enumerate(remaining_cnts):\n",
    "        for cnt in contours_static:\n",
    "            cnt = cnt.reshape(-1, 2)\n",
    "            for x, y in cnt:\n",
    "                if cv2.pointPolygonTest(r_cnt, (float(x), float(y)), True) >= 0:\n",
    "                    cv2.drawContours(mask, [r_cnt], -1, 0, -1)\n",
    "                    remove_close_cnts(i)\n",
    "                    break\n",
    "    \n",
    "    if show_images:\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(anomaly_image)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10.6, 10.6))\n",
    "        plt.imshow(mask)\n",
    "        plt.show()\n",
    "\n",
    "    return anomaly_image * mask\n",
    "\n",
    "def remove_wrong_anomalies(anomaly_image, original_image):\n",
    "    kernel = np.ones((3, 3),np.uint8)\n",
    "    anomaly_image = cv2.morphologyEx(anomaly_image.astype('uint8'), cv2.MORPH_OPEN, kernel)\n",
    "    kernel = np.ones((10, 10),np.uint8)\n",
    "    anomaly_image = cv2.morphologyEx(anomaly_image.astype('uint8'), cv2.MORPH_CLOSE, kernel)\n",
    "    contours, _ = cv2.findContours(anomaly_image.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Remove tiny areas\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        \n",
    "        rect = cv2.boundingRect(cnt)\n",
    "        x,y,w,h = rect\n",
    "        \n",
    "        if area <= 10:\n",
    "            anomaly_image[y:y+h, x:x+w] = 0\n",
    "        else:\n",
    "            mean = original_image.mean(axis=(0,1))\n",
    "            std = original_image.std(axis=(0,1))\n",
    "            if not area_fo_statistics(original_image[y:y+h, x:x+w], mean, std, False):\n",
    "                anomaly_image[y:y+h, x:x+w] = 0\n",
    "                \n",
    "    return anomaly_image\n",
    "\n",
    "def area_fo_statistics(area, mean, std, show_images=False):\n",
    "    m = area.mean(axis=(0, 1))\n",
    "    s = area.std(axis=(0, 1))\n",
    "    \n",
    "    bias = 1.1\n",
    "    \n",
    "    rbh = 1.6\n",
    "    \n",
    "    r_out = (m[0] + s[0] > (mean[0] + std[0]) * bias).all()\n",
    "    b_out = (m[2] + s[2] > (mean[2] + std[2]) * bias).all()\n",
    "    \n",
    "    red_or_blue_high = ((m[0] + s[0])/(mean[0] + std[0]) > rbh or (m[2] + s[2])/(mean[2] + std[2]) > rbh)\n",
    "    \n",
    "    quot = (m + s)/(mean + std)\n",
    "    green_min = False\n",
    "    if np.argmin(quot) == 1: \n",
    "        green_min = True\n",
    "\n",
    "    if (r_out and b_out and green_min) or red_or_blue_high:\n",
    "        if show_images:    \n",
    "            print(mean + std)\n",
    "            print(m + s)\n",
    "            print((m + s)/(mean + std))\n",
    "            print()\n",
    "            plt.imshow(area)\n",
    "            plt.show()\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634338db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_strings = {0: \"valid-1-0\", 1: \"valid-1-1\", 2: \"valid-1-2\", 3: \"valid-1-3\", 4: \"valid-1-4\", 5: \"valid-1-5\",\n",
    "                 6: \"valid-1-6\", 7: \"valid-2-0\", 8: \"valid-2-1\", 9: \"valid-2-2\", 10: \"valid-2-3\"}\n",
    "\n",
    "bb_pred_dict = {}\n",
    "\n",
    "mean, std = analyze_images(valid_images)\n",
    "\n",
    "for j in valid_strings:\n",
    "    print(valid_strings[j])\n",
    "    cleared_image = clear_anomaly_new(valid_images, valid_strings[j], show_images=False, threshold=threshold)\n",
    "    bb_pred_dict[valid_strings[j]] = draw_bounding_boxes_new(cleared_image, valid_images, valid_strings[j], show_images=True, add_ground_truth=True)\n",
    "\n",
    "result = str(bb_pred_dict)\n",
    "result = result.replace(\"'\", \"\\\"\")\n",
    "print(result)\n",
    "with open(\"val_fail.json\",\"w\") as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(detections: Dict[str, List[BoundingBox]],\n",
    "             targets: Dict[str, List[BoundingBox]]) -> float:\n",
    "    return compute_AP(detections, targets)\n",
    "\n",
    "\n",
    "# load the detections\n",
    "detections = read_bb(\"val_fail.json\")\n",
    "# load the targets\n",
    "targets = read_bb(os.path.join('../4_Notebooks/data_WiSAR/data/validation/', 'labels.json'))\n",
    "\n",
    "ap = evaluate(detections, targets)\n",
    "print(f\"Average precision={ap:.5f} on validation set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77aa84e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_strings = {0: \"test-1-7\", 1: \"test-1-8\", 2: \"test-1-9\", 3: \"test-1-10\", 4: \"test-1-11\", 5: \"test-1-12\",\n",
    "                 6: \"test-1-13\", 7: \"test-1-14\", 8: \"test-2-4\", 9: \"test-2-5\", 10: \"test-2-6\", 11: \"test-2-7\", \n",
    "                 12: \"test-2-8\"}\n",
    "\n",
    "bb_pred_dict = {}\n",
    "\n",
    "mean, std = analyze_images(test_images)\n",
    "\n",
    "for j in test_strings:\n",
    "    print(test_strings[j])\n",
    "    cleared_image = clear_anomaly_new(test_images, test_strings[j], show_images=False, threshold=threshold)\n",
    "    bb_pred_dict[test_strings[j]] = draw_bounding_boxes_new(cleared_image, test_images, test_strings[j], show_images=True, add_ground_truth=True)\n",
    "\n",
    "result = str(bb_pred_dict)\n",
    "result = result.replace(\"'\", \"\\\"\")\n",
    "print(result)\n",
    "with open(\"test_fail.json\",\"w\") as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b14ab",
   "metadata": {},
   "source": [
    "# rCRD approach for finding anomalies (also didn't improve results, code added for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d45de8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rCRD_XY(image, r, l, threshold):\n",
    "    image = image.swapaxes(2, 1).swapaxes(0, 1)\n",
    "\n",
    "    d = image.shape[0]\n",
    "    n = image.shape[1]*image.shape[2]\n",
    "    \n",
    "    X = image.copy()\n",
    "    Xr = np.zeros([d, r])\n",
    "    \n",
    "    pts = []\n",
    "                \n",
    "    for i in range(r):\n",
    "        x = np.random.randint(0, image.shape[1])\n",
    "        y = np.random.randint(0, image.shape[2])\n",
    "                \n",
    "        while all(X[:, x, y] == 0) or ((y, x) in pts):\n",
    "            x = np.random.randint(0, image.shape[1])\n",
    "            y = np.random.randint(0, image.shape[2])\n",
    "        Xr[:, i] = X[:, x, y]\n",
    "        pts.append((y, x))\n",
    "        \n",
    "    a = Xr.T @ Xr + l\n",
    "    unq, count = np.unique(a, axis=0, return_counts=True)\n",
    "    b = np.linalg.pinv(Xr.T @ Xr + l) @ Xr.T\n",
    "    A = (X.T @ b.T).T\n",
    "    \n",
    "    X_est = (A.T @ Xr.T).T\n",
    "    \n",
    "    diff = X - X_est\n",
    "        \n",
    "    delta = np.linalg.norm(diff, axis=0)\n",
    "    delta = delta/np.max(delta)\n",
    "    anomaly = np.where(delta > threshold, 1, 0)\n",
    "    \n",
    "    return anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326dadb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image = valid_images['valid-2-0-6']\n",
    "plt.figure(figsize=(10.6, 10.6))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "r = 500\n",
    "t = 0.8\n",
    "\n",
    "anomaly = rCRD_XY(image, r=r, l=1e-6, threshold=t)\n",
    "\n",
    "plt.figure(figsize=(10.6, 10.6))\n",
    "plt.imshow(anomaly)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
